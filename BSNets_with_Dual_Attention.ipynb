{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BSNet with Dual Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucalyptus/BS-Nets-Implementation-Pytorch/blob/master/BSNets_with_Dual_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aySwtTTLdACS",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import io \n",
        "import torch.utils.data\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_vZ-V7Cfffp",
        "outputId": "095faf56-04bf-4be7-9e94-c4b4e7f64ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install -U spectral\n",
        "!pip install pytorch_ssim\n",
        "from pytorch_ssim import ssim\n",
        "if not (os.path.isfile('/content/Indian_pines_corrected.mat')):\n",
        "  !wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n",
        "if not (os.path.isfile('/content/Indian_pines_gt.mat')):\n",
        "  !wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spectral in /usr/local/lib/python3.6/dist-packages (0.20)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from spectral) (1.17.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Ap1zqDEf5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n",
        "    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjCQsohnvrEF",
        "colab": {}
      },
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "\n",
        "    ## From: https://github.com/gokriznastic/HybridSN/blob/master/Hybrid-Spectral-Net.ipynb\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX\n",
        "\n",
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
        "\n",
        "     ## From: https://github.com/gokriznastic/HybridSN/blob/master/Hybrid-Spectral-Net.ipynb\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]), dtype=np.uint8)\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]), dtype=np.uint8)\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sYgdv3VZw2mz",
        "colab": {}
      },
      "source": [
        "class HyperSpectralDataset(Dataset):\n",
        "    \"\"\"HyperSpectral dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,data_url,label_url):\n",
        "        \n",
        "        self.data = np.array(scipy.io.loadmat('/content/'+data_url.split('/')[-1])[data_url.split('/')[-1].split('.')[0].lower()])\n",
        "        self.targets = np.array(scipy.io.loadmat('/content/'+label_url.split('/')[-1])[label_url.split('/')[-1].split('.')[0].lower()])\n",
        "        self.data, self.targets = createImageCubes(self.data,self.targets, windowSize=5)\n",
        "        \n",
        "        #self.data = self.data.reshape((-1, self.data.shape[1], self.data.shape[2], self.data.shape[3], 1))\n",
        "        \n",
        "        self.data = self.data[:10240,:,:,:]\n",
        "        self.targets = self.targets[:10240]\n",
        "        self.data = torch.Tensor(self.data)\n",
        "        self.data = self.data.permute(0,3,1,2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      \n",
        "      return self.data[idx,:,:,:] , self.targets[idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJY-1XbQbb64",
        "colab": {}
      },
      "source": [
        "data_train = HyperSpectralDataset('Indian_pines_corrected.mat','Indian_pines_gt.mat')\n",
        "train_loader = DataLoader(data_train, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SCfG3YT5X9Ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "89fc1da6-b256-4057-d490-a88518d3e4c6"
      },
      "source": [
        "\"\"\"class BAM(nn.Module):\n",
        "    def __init__(self):\n",
        "      \n",
        "        super(BAM, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(200, 64, (3, 3), 1, 0),\n",
        "                                   nn.ReLU(True))\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(64,128),\n",
        "                                 nn.ReLU(True))\n",
        "    \n",
        "        self.fc2 = nn.Sequential(nn.Linear(128,200),\n",
        "                                 nn.Sigmoid())\n",
        "                    \n",
        "    def forward(self,x):\n",
        "            \n",
        "        x = self.conv1(x)\n",
        "        print(x.shape)\n",
        "        x = F.avg_pool2d(x, x.size()[2:4])\n",
        "        print(x.shape)\n",
        "        x = x.view(-1, 64)\n",
        "        print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        print(x.shape)\n",
        "        return x.unsqueeze(2).unsqueeze(3)     \"\"\""
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'class BAM(nn.Module):\\n    def __init__(self):\\n      \\n        super(BAM, self).__init__()\\n        self.conv1 = nn.Sequential(nn.Conv2d(200, 64, (3, 3), 1, 0),\\n                                   nn.ReLU(True))\\n\\n        self.fc1 = nn.Sequential(nn.Linear(64,128),\\n                                 nn.ReLU(True))\\n    \\n        self.fc2 = nn.Sequential(nn.Linear(128,200),\\n                                 nn.Sigmoid())\\n                    \\n    def forward(self,x):\\n            \\n        x = self.conv1(x)\\n        print(x.shape)\\n        x = F.avg_pool2d(x, x.size()[2:4])\\n        print(x.shape)\\n        x = x.view(-1, 64)\\n        print(x.shape)\\n        x = self.fc1(x)\\n        \\n        x = self.fc2(x)\\n        print(x.shape)\\n        return x.unsqueeze(2).unsqueeze(3)     '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2nYOMk53X9Eg",
        "colab": {}
      },
      "source": [
        "\"\"\"class BSNET_Conv(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(BSNET_Conv, self).__init__()\n",
        "        self.BAM = BAM()\n",
        "        self.RecNet = RecNet()\n",
        "\n",
        "    def forward(self,x):\n",
        "        #print('before bam ', x.shape)\n",
        "        BRW = self.BAM(x)\n",
        "        x = x * BRW\n",
        "        #print('after bam ',x.shape)\n",
        "        \n",
        "        x = x.unsqueeze(1)\n",
        "        #print(x.shape)\n",
        "        ret = self.RecNet(x)\n",
        "        #print('after reconstruction', ret.shape)\n",
        "        \n",
        "        return ret\"\"\"\n",
        "model = BSNET_Conv().to(device)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUeiQr1GFgCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PAM_Module(Module):\n",
        "    \"\"\" Position attention module  https://github.com/junfu1115/DANet/blob/master/encoding/nn/attention.py\"\"\"\n",
        "    #Ref from SAGAN\n",
        "    def __init__(self, in_dim):\n",
        "        super(PAM_Module, self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "\n",
        "        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        \n",
        "        self.gamma = Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X H X W)\n",
        "            returns :\n",
        "                out : attention value + input feature\n",
        "                attention: B X (HxW) X (HxW)\n",
        "        \"\"\"\n",
        "        m_batchsize, C, height, width = x.size()\n",
        "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
        "\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(m_batchsize, C, height, width)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        #out = F.avg_pool2d(out, out.size()[2:4])\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class CAM_Module(Module):\n",
        "    \"\"\" Channel attention module https://github.com/junfu1115/DANet/blob/master/encoding/nn/attention.py\"\"\"\n",
        "    def __init__(self):\n",
        "        super(CAM_Module, self).__init__()\n",
        "        #self.chanel_in = in_dim\n",
        "        \n",
        "\n",
        "\n",
        "        self.gamma = Parameter(torch.zeros(1))\n",
        "        self.softmax  = Softmax(dim=-1)\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X H X W)\n",
        "            returns :\n",
        "                out : attention value + input feature\n",
        "                attention: B X C X C\n",
        "        \"\"\"\n",
        "        m_batchsize, C, height, width = x.size()\n",
        "        proj_query = x.view(m_batchsize, C, -1)\n",
        "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
        "        attention = self.softmax(energy_new)\n",
        "        proj_value = x.view(m_batchsize, C, -1)\n",
        "\n",
        "        out = torch.bmm(attention, proj_value)\n",
        "        out = out.view(m_batchsize, C, height, width)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        #out = F.avg_pool2d(out, out.size()[2:4])\n",
        "        \n",
        "        \n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q47qdlizX9EY",
        "colab": {}
      },
      "source": [
        "class RecNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RecNet, self).__init__()\n",
        "        self.conv3d_1 = nn.Sequential(nn.Conv3d(1, 24, (24, 3, 3), 1),\n",
        "                        nn.BatchNorm3d(24),\n",
        "                        nn.PReLU())\n",
        "        \n",
        "        self.conv3d_2 = nn.Sequential(nn.Conv3d(24, 48, (24, 3, 3), 1),\n",
        "                        nn.BatchNorm3d(48),\n",
        "                        nn.PReLU())\n",
        "                        \n",
        "        \n",
        "        self.pool3d = nn.MaxPool3d((18, 1, 1), (18, 1, 1))\n",
        "        \n",
        "        self.deconv3d_1 = nn.Sequential(nn.ConvTranspose3d(48, 24, (9, 3, 3), (22, 1, 1)),\n",
        "                          nn.BatchNorm3d(24),\n",
        "                          nn.PReLU())\n",
        "        \n",
        "        self.deconv3d_2 = nn.Sequential(nn.ConvTranspose3d(24, 1, (38, 3, 3), (1, 1, 1)),\n",
        "                          nn.BatchNorm3d(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv3d_1(x)\n",
        "        x = self.conv3d_2(x)\n",
        "        \n",
        "        x = self.pool3d(x)\n",
        "        \n",
        "        x = self.deconv3d_1(x)\n",
        "        x = self.deconv3d_2(x)\n",
        "        \n",
        "        return x.squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8VpTQM2ot7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DANet(Module):\n",
        "  def __init__(self):\n",
        "    super(DANet,self).__init__()\n",
        "    self.PAM_Module = PAM_Module(200)\n",
        "    self.CAM_Module = CAM_Module()\n",
        "    self.RecNet = RecNet()\n",
        "  def forward(self,x):\n",
        "    \n",
        "    P = self.PAM_Module(x)\n",
        "    C = self.CAM_Module(x)\n",
        "    #B,Ch,H,W = P.size()\n",
        "    J = P + C\n",
        "    J =  J.unsqueeze(1)\n",
        "    ret = self.RecNet(J)\n",
        "    \n",
        "    \n",
        "    return ret\n",
        "    \n",
        "    \n",
        "danet_model = DANet().to(device)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rOGMoXZlY0Ss",
        "colab": {}
      },
      "source": [
        "def psnr(x_true, x_pred):\n",
        "    n_samples = x_true.shape[0]\n",
        "    n_bands = x_true.shape[1]\n",
        "    PSNR = np.zeros(n_bands)\n",
        "    MSE = np.zeros(n_bands)\n",
        "    mask = np.ones(n_bands)\n",
        "    for k in range(n_bands):\n",
        "        x_true_k = x_true[:, k].reshape([-1])\n",
        "        x_pred_k = x_pred[:, k].reshape([-1])\n",
        "        MSE[k] = 1.0 / n_samples * mean_squared_error(x_true_k, x_pred_k, )\n",
        "        MAX_k = np.max(x_true_k)\n",
        "        if MAX_k != 0:\n",
        "            PSNR[k] = 10 * math.log10(math.pow(MAX_k, 2) / MSE[k])\n",
        "        else:\n",
        "            mask[k] = 0\n",
        "\n",
        "    psnr = PSNR.sum()/mask.sum()\n",
        "    mse = MSE.mean()\n",
        "    print('psnr', psnr)\n",
        "    print('mse', mse)\n",
        "    \n",
        "    return psnr, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_UVWhGROX9Ei",
        "colab": {}
      },
      "source": [
        "#model = BSNET_Conv().to(device) \n",
        "\n",
        "optimizer = optim.SGD(danet_model.parameters(), lr=0.002, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mFUiHpJyX9Em",
        "outputId": "898e26a8-e6c7-4a93-e23c-6a7670fabb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train(epoch):\n",
        "    danet_model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = danet_model(data)\n",
        "#         print(output.shape, data.shape)\n",
        "        \n",
        "        loss = F.l1_loss(output,data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 50 == 0:\n",
        "          \n",
        "            x_true = data.detach().cpu().numpy()\n",
        "            x_predict = output.detach().cpu().numpy()\n",
        "            x_pred_centre = x_predict[:, :, 2, 2]\n",
        "            x_true_centre = x_true[:, :, 2, 2]\n",
        "            psnr(x_true_centre, x_pred_centre)\n",
        "        \n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        \n",
        "        #if batch_idx % 100 == 0:\n",
        "          #print(output.detach().cpu().numpy().shape)\n",
        "            \n",
        "\n",
        "def test():\n",
        "    with torch.no_grad():\n",
        "        danet_model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = danet_model(data)\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += F.mse_loss(output, target).item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "              .format(test_loss, correct, len(test_loader.dataset),\n",
        "                      100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "for epoch in range(1, 40 + 1):\n",
        "    train(epoch)\n",
        "    #test()"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "psnr 23.204136186580573\n",
            "mse 308.616954870224\n",
            "Train Epoch: 1 [0/10240 (0%)]\tLoss: 119.135925\n",
            "psnr 23.1119640668545\n",
            "mse 314.63019690513613\n",
            "Train Epoch: 1 [3200/10240 (31%)]\tLoss: 119.170303\n",
            "psnr 23.23874639175122\n",
            "mse 305.86190910339354\n",
            "Train Epoch: 1 [6400/10240 (62%)]\tLoss: 117.249702\n",
            "psnr 23.215297499212816\n",
            "mse 302.82755693435666\n",
            "Train Epoch: 1 [9600/10240 (94%)]\tLoss: 117.757347\n",
            "psnr 23.255090482952646\n",
            "mse 299.50838470458984\n",
            "Train Epoch: 2 [0/10240 (0%)]\tLoss: 115.886147\n",
            "psnr 23.35677429961558\n",
            "mse 297.2118961906433\n",
            "Train Epoch: 2 [3200/10240 (31%)]\tLoss: 114.953873\n",
            "psnr 23.43951715247009\n",
            "mse 291.64262910842893\n",
            "Train Epoch: 2 [6400/10240 (62%)]\tLoss: 114.043610\n",
            "psnr 23.452602081013154\n",
            "mse 287.484260597229\n",
            "Train Epoch: 2 [9600/10240 (94%)]\tLoss: 113.719299\n",
            "psnr 23.54886758553451\n",
            "mse 286.53689643859866\n",
            "Train Epoch: 3 [0/10240 (0%)]\tLoss: 114.005379\n",
            "psnr 23.42139574256693\n",
            "mse 288.3711777591705\n",
            "Train Epoch: 3 [3200/10240 (31%)]\tLoss: 112.883614\n",
            "psnr 23.559123986951327\n",
            "mse 281.70388145446776\n",
            "Train Epoch: 3 [6400/10240 (62%)]\tLoss: 111.454674\n",
            "psnr 23.628482217477984\n",
            "mse 276.86778133392335\n",
            "Train Epoch: 3 [9600/10240 (94%)]\tLoss: 109.820999\n",
            "psnr 23.62374674574582\n",
            "mse 276.4517353439331\n",
            "Train Epoch: 4 [0/10240 (0%)]\tLoss: 110.088150\n",
            "psnr 23.627054519901602\n",
            "mse 279.0852673530579\n",
            "Train Epoch: 4 [3200/10240 (31%)]\tLoss: 109.877625\n",
            "psnr 23.718352149394388\n",
            "mse 272.15919989585876\n",
            "Train Epoch: 4 [6400/10240 (62%)]\tLoss: 110.034660\n",
            "psnr 23.746903157481448\n",
            "mse 272.093681678772\n",
            "Train Epoch: 4 [9600/10240 (94%)]\tLoss: 107.969460\n",
            "psnr 23.77648067858097\n",
            "mse 271.5836813926697\n",
            "Train Epoch: 5 [0/10240 (0%)]\tLoss: 108.437241\n",
            "psnr 23.843455734209346\n",
            "mse 267.62526463508607\n",
            "Train Epoch: 5 [3200/10240 (31%)]\tLoss: 108.133072\n",
            "psnr 23.779071489290217\n",
            "mse 269.7297765350342\n",
            "Train Epoch: 5 [6400/10240 (62%)]\tLoss: 107.232353\n",
            "psnr 23.940472504980562\n",
            "mse 261.8976565933228\n",
            "Train Epoch: 5 [9600/10240 (94%)]\tLoss: 106.009125\n",
            "psnr 23.926654022997678\n",
            "mse 259.8370475101471\n",
            "Train Epoch: 6 [0/10240 (0%)]\tLoss: 106.009903\n",
            "psnr 23.931572084582296\n",
            "mse 259.20241807937623\n",
            "Train Epoch: 6 [3200/10240 (31%)]\tLoss: 104.291901\n",
            "psnr 23.991229612254266\n",
            "mse 256.1820326089859\n",
            "Train Epoch: 6 [6400/10240 (62%)]\tLoss: 105.343971\n",
            "psnr 24.066350534636886\n",
            "mse 255.4893472290039\n",
            "Train Epoch: 6 [9600/10240 (94%)]\tLoss: 103.947456\n",
            "psnr 24.06404759462833\n",
            "mse 252.45665458679198\n",
            "Train Epoch: 7 [0/10240 (0%)]\tLoss: 103.217598\n",
            "psnr 24.114589718532145\n",
            "mse 253.4181116771698\n",
            "Train Epoch: 7 [3200/10240 (31%)]\tLoss: 102.961838\n",
            "psnr 24.110575105810767\n",
            "mse 252.3081834936142\n",
            "Train Epoch: 7 [6400/10240 (62%)]\tLoss: 102.748337\n",
            "psnr 24.260072426151165\n",
            "mse 244.6159973049164\n",
            "Train Epoch: 7 [9600/10240 (94%)]\tLoss: 100.423576\n",
            "psnr 24.284876837628644\n",
            "mse 240.243306388855\n",
            "Train Epoch: 8 [0/10240 (0%)]\tLoss: 100.914413\n",
            "psnr 24.273100165965726\n",
            "mse 240.6777722454071\n",
            "Train Epoch: 8 [3200/10240 (31%)]\tLoss: 100.221252\n",
            "psnr 24.32659888934122\n",
            "mse 241.8056303215027\n",
            "Train Epoch: 8 [6400/10240 (62%)]\tLoss: 99.509781\n",
            "psnr 24.34945127985588\n",
            "mse 237.9091663837433\n",
            "Train Epoch: 8 [9600/10240 (94%)]\tLoss: 99.545425\n",
            "psnr 24.382699439811667\n",
            "mse 236.32051378250122\n",
            "Train Epoch: 9 [0/10240 (0%)]\tLoss: 98.734062\n",
            "psnr 24.496754684897024\n",
            "mse 231.2228946018219\n",
            "Train Epoch: 9 [3200/10240 (31%)]\tLoss: 97.654274\n",
            "psnr 24.512809771055682\n",
            "mse 231.53823829650878\n",
            "Train Epoch: 9 [6400/10240 (62%)]\tLoss: 97.540207\n",
            "psnr 24.651345296033952\n",
            "mse 225.7660426902771\n",
            "Train Epoch: 9 [9600/10240 (94%)]\tLoss: 96.559059\n",
            "psnr 24.719492892991422\n",
            "mse 219.81103823184966\n",
            "Train Epoch: 10 [0/10240 (0%)]\tLoss: 97.359886\n",
            "psnr 24.626476345853252\n",
            "mse 225.29598739147187\n",
            "Train Epoch: 10 [3200/10240 (31%)]\tLoss: 96.176498\n",
            "psnr 24.629938640040553\n",
            "mse 222.20038558483122\n",
            "Train Epoch: 10 [6400/10240 (62%)]\tLoss: 95.190514\n",
            "psnr 24.706638041619364\n",
            "mse 220.27811327457428\n",
            "Train Epoch: 10 [9600/10240 (94%)]\tLoss: 95.360527\n",
            "psnr 24.70577472685703\n",
            "mse 222.56456298828124\n",
            "Train Epoch: 11 [0/10240 (0%)]\tLoss: 94.372734\n",
            "psnr 24.814067033211167\n",
            "mse 214.66125582218172\n",
            "Train Epoch: 11 [3200/10240 (31%)]\tLoss: 94.509651\n",
            "psnr 24.869815918264116\n",
            "mse 215.14775968551635\n",
            "Train Epoch: 11 [6400/10240 (62%)]\tLoss: 93.989609\n",
            "psnr 24.88640278223764\n",
            "mse 214.47870104312898\n",
            "Train Epoch: 11 [9600/10240 (94%)]\tLoss: 93.089027\n",
            "psnr 24.94100705953518\n",
            "mse 211.28254245758058\n",
            "Train Epoch: 12 [0/10240 (0%)]\tLoss: 93.372490\n",
            "psnr 24.94288783298733\n",
            "mse 209.54128213882447\n",
            "Train Epoch: 12 [3200/10240 (31%)]\tLoss: 90.887260\n",
            "psnr 25.030404224208144\n",
            "mse 204.9946410575509\n",
            "Train Epoch: 12 [6400/10240 (62%)]\tLoss: 91.664139\n",
            "psnr 25.13211203929447\n",
            "mse 204.151102104187\n",
            "Train Epoch: 12 [9600/10240 (94%)]\tLoss: 91.136574\n",
            "psnr 25.062945597935666\n",
            "mse 202.87496361732482\n",
            "Train Epoch: 13 [0/10240 (0%)]\tLoss: 90.143524\n",
            "psnr 25.16825339692275\n",
            "mse 199.3111846101284\n",
            "Train Epoch: 13 [3200/10240 (31%)]\tLoss: 90.356644\n",
            "psnr 25.14611298520456\n",
            "mse 199.44102782726287\n",
            "Train Epoch: 13 [6400/10240 (62%)]\tLoss: 89.245995\n",
            "psnr 25.213943213701942\n",
            "mse 199.12750664949417\n",
            "Train Epoch: 13 [9600/10240 (94%)]\tLoss: 89.789459\n",
            "psnr 25.157208281960003\n",
            "mse 199.38168624162674\n",
            "Train Epoch: 14 [0/10240 (0%)]\tLoss: 89.703972\n",
            "psnr 25.361074277223008\n",
            "mse 192.85608869552613\n",
            "Train Epoch: 14 [3200/10240 (31%)]\tLoss: 88.631447\n",
            "psnr 25.235734328195527\n",
            "mse 195.33942101359366\n",
            "Train Epoch: 14 [6400/10240 (62%)]\tLoss: 87.955002\n",
            "psnr 25.48404341281369\n",
            "mse 190.01760702610017\n",
            "Train Epoch: 14 [9600/10240 (94%)]\tLoss: 87.405975\n",
            "psnr 25.364963769129744\n",
            "mse 190.3193107521534\n",
            "Train Epoch: 15 [0/10240 (0%)]\tLoss: 87.109940\n",
            "psnr 25.54719966726918\n",
            "mse 189.43709714889528\n",
            "Train Epoch: 15 [3200/10240 (31%)]\tLoss: 86.542084\n",
            "psnr 25.561835136174896\n",
            "mse 183.86043832063675\n",
            "Train Epoch: 15 [6400/10240 (62%)]\tLoss: 85.593590\n",
            "psnr 25.545124041190046\n",
            "mse 187.50477073669433\n",
            "Train Epoch: 15 [9600/10240 (94%)]\tLoss: 85.840996\n",
            "psnr 25.540667277517567\n",
            "mse 183.30939996004105\n",
            "Train Epoch: 16 [0/10240 (0%)]\tLoss: 85.212372\n",
            "psnr 25.62214326845299\n",
            "mse 180.80469040870668\n",
            "Train Epoch: 16 [3200/10240 (31%)]\tLoss: 85.572411\n",
            "psnr 25.583465016568283\n",
            "mse 183.84582968235017\n",
            "Train Epoch: 16 [6400/10240 (62%)]\tLoss: 85.262810\n",
            "psnr 25.63183564082544\n",
            "mse 183.4980320739746\n",
            "Train Epoch: 16 [9600/10240 (94%)]\tLoss: 83.351799\n",
            "psnr 25.75416626747816\n",
            "mse 176.95028468847275\n",
            "Train Epoch: 17 [0/10240 (0%)]\tLoss: 84.097687\n",
            "psnr 25.90819425189915\n",
            "mse 171.96448122382165\n",
            "Train Epoch: 17 [3200/10240 (31%)]\tLoss: 83.246735\n",
            "psnr 25.796417050689957\n",
            "mse 176.34956560492515\n",
            "Train Epoch: 17 [6400/10240 (62%)]\tLoss: 82.395561\n",
            "psnr 25.861398948521828\n",
            "mse 173.94328310668467\n",
            "Train Epoch: 17 [9600/10240 (94%)]\tLoss: 82.626328\n",
            "psnr 25.884871582130113\n",
            "mse 171.45665840864183\n",
            "Train Epoch: 18 [0/10240 (0%)]\tLoss: 82.043419\n",
            "psnr 25.82586948590246\n",
            "mse 173.78606835246086\n",
            "Train Epoch: 18 [3200/10240 (31%)]\tLoss: 81.917252\n",
            "psnr 25.98639469501213\n",
            "mse 167.38187622129917\n",
            "Train Epoch: 18 [6400/10240 (62%)]\tLoss: 80.651337\n",
            "psnr 26.154738218643843\n",
            "mse 164.82827335357666\n",
            "Train Epoch: 18 [9600/10240 (94%)]\tLoss: 79.530479\n",
            "psnr 26.034230388795926\n",
            "mse 167.1228800177574\n",
            "Train Epoch: 19 [0/10240 (0%)]\tLoss: 80.481262\n",
            "psnr 26.030408701116688\n",
            "mse 168.06404571294786\n",
            "Train Epoch: 19 [3200/10240 (31%)]\tLoss: 80.677963\n",
            "psnr 26.272161801072354\n",
            "mse 162.1062509524822\n",
            "Train Epoch: 19 [6400/10240 (62%)]\tLoss: 79.411423\n",
            "psnr 26.07711689751034\n",
            "mse 164.95449601083993\n",
            "Train Epoch: 19 [9600/10240 (94%)]\tLoss: 79.641876\n",
            "psnr 26.10846794554258\n",
            "mse 161.9523465538025\n",
            "Train Epoch: 20 [0/10240 (0%)]\tLoss: 78.970573\n",
            "psnr 26.23475796587535\n",
            "mse 160.81069518089294\n",
            "Train Epoch: 20 [3200/10240 (31%)]\tLoss: 78.015091\n",
            "psnr 26.335625876058334\n",
            "mse 156.6757235956192\n",
            "Train Epoch: 20 [6400/10240 (62%)]\tLoss: 77.174019\n",
            "psnr 26.213777304507463\n",
            "mse 158.1623749303818\n",
            "Train Epoch: 20 [9600/10240 (94%)]\tLoss: 78.138351\n",
            "psnr 26.39533143881001\n",
            "mse 155.73506716251373\n",
            "Train Epoch: 21 [0/10240 (0%)]\tLoss: 77.563286\n",
            "psnr 26.41517202881161\n",
            "mse 154.82734601795673\n",
            "Train Epoch: 21 [3200/10240 (31%)]\tLoss: 76.971848\n",
            "psnr 26.41219889277162\n",
            "mse 154.3420023268461\n",
            "Train Epoch: 21 [6400/10240 (62%)]\tLoss: 76.263420\n",
            "psnr 26.782535692519993\n",
            "mse 146.86998936653137\n",
            "Train Epoch: 21 [9600/10240 (94%)]\tLoss: 76.240547\n",
            "psnr 26.630177692014236\n",
            "mse 149.52437432646752\n",
            "Train Epoch: 22 [0/10240 (0%)]\tLoss: 75.759354\n",
            "psnr 26.675622436162154\n",
            "mse 146.5454792881012\n",
            "Train Epoch: 22 [3200/10240 (31%)]\tLoss: 76.135460\n",
            "psnr 26.740396050424746\n",
            "mse 142.88944541454316\n",
            "Train Epoch: 22 [6400/10240 (62%)]\tLoss: 73.622475\n",
            "psnr 26.894799572641542\n",
            "mse 140.27728846490382\n",
            "Train Epoch: 22 [9600/10240 (94%)]\tLoss: 75.135796\n",
            "psnr 26.670021034453384\n",
            "mse 146.7354827451706\n",
            "Train Epoch: 23 [0/10240 (0%)]\tLoss: 74.510742\n",
            "psnr 26.823501674953867\n",
            "mse 141.7535516500473\n",
            "Train Epoch: 23 [3200/10240 (31%)]\tLoss: 73.564423\n",
            "psnr 26.829577257756256\n",
            "mse 142.60401334822177\n",
            "Train Epoch: 23 [6400/10240 (62%)]\tLoss: 73.813736\n",
            "psnr 26.862286738786725\n",
            "mse 140.30244166493415\n",
            "Train Epoch: 23 [9600/10240 (94%)]\tLoss: 73.720741\n",
            "psnr 26.697306523159476\n",
            "mse 143.95507435202597\n",
            "Train Epoch: 24 [0/10240 (0%)]\tLoss: 73.629379\n",
            "psnr 27.05959251098164\n",
            "mse 134.53854699015616\n",
            "Train Epoch: 24 [3200/10240 (31%)]\tLoss: 72.831177\n",
            "psnr 27.12543756607657\n",
            "mse 134.30763124644756\n",
            "Train Epoch: 24 [6400/10240 (62%)]\tLoss: 71.950165\n",
            "psnr 27.175546141034975\n",
            "mse 131.5485382425785\n",
            "Train Epoch: 24 [9600/10240 (94%)]\tLoss: 71.547142\n",
            "psnr 26.974564638752483\n",
            "mse 137.71350890874862\n",
            "Train Epoch: 25 [0/10240 (0%)]\tLoss: 71.700653\n",
            "psnr 27.162330345037223\n",
            "mse 133.37289096593858\n",
            "Train Epoch: 25 [3200/10240 (31%)]\tLoss: 71.192627\n",
            "psnr 26.891189112228904\n",
            "mse 138.2500296330452\n",
            "Train Epoch: 25 [6400/10240 (62%)]\tLoss: 71.638237\n",
            "psnr 26.971743896307444\n",
            "mse 136.1513080406189\n",
            "Train Epoch: 25 [9600/10240 (94%)]\tLoss: 70.903069\n",
            "psnr 27.611205865549536\n",
            "mse 124.726883456707\n",
            "Train Epoch: 26 [0/10240 (0%)]\tLoss: 70.148087\n",
            "psnr 27.384103342426396\n",
            "mse 126.67523408532142\n",
            "Train Epoch: 26 [3200/10240 (31%)]\tLoss: 70.287109\n",
            "psnr 27.198979779411573\n",
            "mse 131.1894139122963\n",
            "Train Epoch: 26 [6400/10240 (62%)]\tLoss: 69.995041\n",
            "psnr 27.383385076715175\n",
            "mse 125.19841620922088\n",
            "Train Epoch: 26 [9600/10240 (94%)]\tLoss: 67.903137\n",
            "psnr 27.40796712392002\n",
            "mse 126.25301246464252\n",
            "Train Epoch: 27 [0/10240 (0%)]\tLoss: 67.833313\n",
            "psnr 27.43371639174904\n",
            "mse 126.03640702486038\n",
            "Train Epoch: 27 [3200/10240 (31%)]\tLoss: 68.296112\n",
            "psnr 27.329600579444495\n",
            "mse 127.170522711277\n",
            "Train Epoch: 27 [6400/10240 (62%)]\tLoss: 68.596359\n",
            "psnr 27.160723471496443\n",
            "mse 131.2242248737812\n",
            "Train Epoch: 27 [9600/10240 (94%)]\tLoss: 68.830162\n",
            "psnr 27.6052652922343\n",
            "mse 123.13153341293335\n",
            "Train Epoch: 28 [0/10240 (0%)]\tLoss: 67.312935\n",
            "psnr 27.59285261443706\n",
            "mse 119.46679287075996\n",
            "Train Epoch: 28 [3200/10240 (31%)]\tLoss: 66.502365\n",
            "psnr 27.66742756615177\n",
            "mse 117.90222822666168\n",
            "Train Epoch: 28 [6400/10240 (62%)]\tLoss: 67.509819\n",
            "psnr 27.59578715480594\n",
            "mse 119.46468064546585\n",
            "Train Epoch: 28 [9600/10240 (94%)]\tLoss: 67.331360\n",
            "psnr 27.71910135946554\n",
            "mse 118.58493467569352\n",
            "Train Epoch: 29 [0/10240 (0%)]\tLoss: 66.616562\n",
            "psnr 27.554500250644455\n",
            "mse 119.97569009780884\n",
            "Train Epoch: 29 [3200/10240 (31%)]\tLoss: 65.323067\n",
            "psnr 27.919961470030852\n",
            "mse 116.03079602956772\n",
            "Train Epoch: 29 [6400/10240 (62%)]\tLoss: 66.111641\n",
            "psnr 27.669197770370705\n",
            "mse 119.33532878160477\n",
            "Train Epoch: 29 [9600/10240 (94%)]\tLoss: 66.392891\n",
            "psnr 27.67596576561162\n",
            "mse 118.20661286592484\n",
            "Train Epoch: 30 [0/10240 (0%)]\tLoss: 65.993263\n",
            "psnr 27.932423761332775\n",
            "mse 115.4567859518528\n",
            "Train Epoch: 30 [3200/10240 (31%)]\tLoss: 65.326523\n",
            "psnr 27.67916310119896\n",
            "mse 116.2132552409172\n",
            "Train Epoch: 30 [6400/10240 (62%)]\tLoss: 64.926926\n",
            "psnr 27.814791838873443\n",
            "mse 115.6607076394558\n",
            "Train Epoch: 30 [9600/10240 (94%)]\tLoss: 63.988174\n",
            "psnr 27.969964682310984\n",
            "mse 114.4832504838705\n",
            "Train Epoch: 31 [0/10240 (0%)]\tLoss: 64.483688\n",
            "psnr 28.17352846968015\n",
            "mse 108.28441642284393\n",
            "Train Epoch: 31 [3200/10240 (31%)]\tLoss: 63.975220\n",
            "psnr 28.184243017356142\n",
            "mse 107.79560011148453\n",
            "Train Epoch: 31 [6400/10240 (62%)]\tLoss: 64.593384\n",
            "psnr 28.113762609160236\n",
            "mse 108.69766803860665\n",
            "Train Epoch: 31 [9600/10240 (94%)]\tLoss: 62.146862\n",
            "psnr 28.01791511735467\n",
            "mse 111.82906158566475\n",
            "Train Epoch: 32 [0/10240 (0%)]\tLoss: 63.126949\n",
            "psnr 28.224288962389924\n",
            "mse 106.66188223063946\n",
            "Train Epoch: 32 [3200/10240 (31%)]\tLoss: 63.044170\n",
            "psnr 28.40908330119554\n",
            "mse 104.2322437608242\n",
            "Train Epoch: 32 [6400/10240 (62%)]\tLoss: 63.265537\n",
            "psnr 28.40116098930007\n",
            "mse 103.23777557075023\n",
            "Train Epoch: 32 [9600/10240 (94%)]\tLoss: 62.678288\n",
            "psnr 27.93555143659002\n",
            "mse 112.94416029572487\n",
            "Train Epoch: 33 [0/10240 (0%)]\tLoss: 63.093063\n",
            "psnr 28.428660155762426\n",
            "mse 102.84317931711674\n",
            "Train Epoch: 33 [3200/10240 (31%)]\tLoss: 61.669170\n",
            "psnr 28.4871789815276\n",
            "mse 102.18292240858078\n",
            "Train Epoch: 33 [6400/10240 (62%)]\tLoss: 62.144268\n",
            "psnr 28.210554045972394\n",
            "mse 106.14710706710815\n",
            "Train Epoch: 33 [9600/10240 (94%)]\tLoss: 61.623413\n",
            "psnr 28.195937458393875\n",
            "mse 106.29078869462013\n",
            "Train Epoch: 34 [0/10240 (0%)]\tLoss: 61.703587\n",
            "psnr 28.34877031485747\n",
            "mse 102.60090390324592\n",
            "Train Epoch: 34 [3200/10240 (31%)]\tLoss: 61.273338\n",
            "psnr 28.06654856049754\n",
            "mse 107.90283424258232\n",
            "Train Epoch: 34 [6400/10240 (62%)]\tLoss: 60.908676\n",
            "psnr 28.89647437308032\n",
            "mse 93.48750762104989\n",
            "Train Epoch: 34 [9600/10240 (94%)]\tLoss: 60.064369\n",
            "psnr 28.751890170243044\n",
            "mse 97.94229427874089\n",
            "Train Epoch: 35 [0/10240 (0%)]\tLoss: 60.446262\n",
            "psnr 28.44803345794994\n",
            "mse 98.4012053656578\n",
            "Train Epoch: 35 [3200/10240 (31%)]\tLoss: 60.657139\n",
            "psnr 28.83951224456795\n",
            "mse 98.21413185834885\n",
            "Train Epoch: 35 [6400/10240 (62%)]\tLoss: 59.920189\n",
            "psnr 28.925207778684307\n",
            "mse 93.42090013504028\n",
            "Train Epoch: 35 [9600/10240 (94%)]\tLoss: 60.143986\n",
            "psnr 28.79164405090384\n",
            "mse 97.72302018165588\n",
            "Train Epoch: 36 [0/10240 (0%)]\tLoss: 58.309326\n",
            "psnr 28.61803507958598\n",
            "mse 98.23788609027862\n",
            "Train Epoch: 36 [3200/10240 (31%)]\tLoss: 59.921455\n",
            "psnr 29.03841837030534\n",
            "mse 88.08956451892853\n",
            "Train Epoch: 36 [6400/10240 (62%)]\tLoss: 58.409248\n",
            "psnr 28.748841679098796\n",
            "mse 94.20247343182564\n",
            "Train Epoch: 36 [9600/10240 (94%)]\tLoss: 58.650455\n",
            "psnr 29.07049941283529\n",
            "mse 91.13071150422097\n",
            "Train Epoch: 37 [0/10240 (0%)]\tLoss: 58.088287\n",
            "psnr 29.577388130567968\n",
            "mse 83.98825251579285\n",
            "Train Epoch: 37 [3200/10240 (31%)]\tLoss: 58.251976\n",
            "psnr 28.344416591163125\n",
            "mse 100.78587441921235\n",
            "Train Epoch: 37 [6400/10240 (62%)]\tLoss: 57.897175\n",
            "psnr 28.733973845901843\n",
            "mse 94.57056172013283\n",
            "Train Epoch: 37 [9600/10240 (94%)]\tLoss: 57.697449\n",
            "psnr 29.111157997181405\n",
            "mse 88.25274485707283\n",
            "Train Epoch: 38 [0/10240 (0%)]\tLoss: 58.133682\n",
            "psnr 29.06276796332902\n",
            "mse 90.83219835162163\n",
            "Train Epoch: 38 [3200/10240 (31%)]\tLoss: 56.939182\n",
            "psnr 29.148590376259108\n",
            "mse 88.6489500039816\n",
            "Train Epoch: 38 [6400/10240 (62%)]\tLoss: 56.995770\n",
            "psnr 29.01721026635723\n",
            "mse 88.83289372324944\n",
            "Train Epoch: 38 [9600/10240 (94%)]\tLoss: 57.277237\n",
            "psnr 29.550066890366164\n",
            "mse 82.73596509993077\n",
            "Train Epoch: 39 [0/10240 (0%)]\tLoss: 55.904213\n",
            "psnr 29.090407701433055\n",
            "mse 88.31127791166305\n",
            "Train Epoch: 39 [3200/10240 (31%)]\tLoss: 57.373852\n",
            "psnr 29.13582684141047\n",
            "mse 87.99142639040947\n",
            "Train Epoch: 39 [6400/10240 (62%)]\tLoss: 56.312424\n",
            "psnr 28.996072261059453\n",
            "mse 89.099918115139\n",
            "Train Epoch: 39 [9600/10240 (94%)]\tLoss: 55.453976\n",
            "psnr 29.093494739248598\n",
            "mse 87.04510296225548\n",
            "Train Epoch: 40 [0/10240 (0%)]\tLoss: 55.358288\n",
            "psnr 29.29529248532702\n",
            "mse 83.77279313147068\n",
            "Train Epoch: 40 [3200/10240 (31%)]\tLoss: 55.278893\n",
            "psnr 28.928279282827752\n",
            "mse 90.77413455128669\n",
            "Train Epoch: 40 [6400/10240 (62%)]\tLoss: 55.761776\n",
            "psnr 29.47416407957366\n",
            "mse 82.68952686607838\n",
            "Train Epoch: 40 [9600/10240 (94%)]\tLoss: 54.927277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKy5hAOzbKAK",
        "colab_type": "code",
        "outputId": "d3ade315-5ca1-4754-e2a3-289b42475ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "import torchsummary\n",
        "torchsummary.summary(danet_model.to(device),(200,5,5))"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1             [-1, 25, 5, 5]           5,025\n",
            "            Conv2d-2             [-1, 25, 5, 5]           5,025\n",
            "           Softmax-3               [-1, 25, 25]               0\n",
            "            Conv2d-4            [-1, 200, 5, 5]          40,200\n",
            "        PAM_Module-5            [-1, 200, 5, 5]               0\n",
            "           Softmax-6             [-1, 200, 200]               0\n",
            "        CAM_Module-7            [-1, 200, 5, 5]               0\n",
            "            Conv3d-8        [-1, 24, 177, 3, 3]           5,208\n",
            "       BatchNorm3d-9        [-1, 24, 177, 3, 3]              48\n",
            "            PReLU-10        [-1, 24, 177, 3, 3]               1\n",
            "           Conv3d-11        [-1, 48, 154, 1, 1]         248,880\n",
            "      BatchNorm3d-12        [-1, 48, 154, 1, 1]              96\n",
            "            PReLU-13        [-1, 48, 154, 1, 1]               1\n",
            "        MaxPool3d-14          [-1, 48, 8, 1, 1]               0\n",
            "  ConvTranspose3d-15        [-1, 24, 163, 3, 3]          93,336\n",
            "      BatchNorm3d-16        [-1, 24, 163, 3, 3]              48\n",
            "            PReLU-17        [-1, 24, 163, 3, 3]               1\n",
            "  ConvTranspose3d-18         [-1, 1, 200, 5, 5]           8,209\n",
            "      BatchNorm3d-19         [-1, 1, 200, 5, 5]               2\n",
            "           RecNet-20            [-1, 200, 5, 5]               0\n",
            "================================================================\n",
            "Total params: 406,080\n",
            "Trainable params: 406,080\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 2.40\n",
            "Params size (MB): 1.55\n",
            "Estimated Total Size (MB): 3.97\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZAEnayzfYKO",
        "colab": {}
      },
      "source": [
        "\"\"\"import spectral\n",
        "data_url , label_url = 'Indian_pines_corrected.mat' ,'Indian_pines_gt.mat'\n",
        "X = np.array(scipy.io.loadmat('/content/'+data_url.split('/')[-1])[data_url.split('/')[-1].split('.')[0].lower()])\n",
        "y = np.array(scipy.io.loadmat('/content/'+label_url.split('/')[-1])[label_url.split('/')[-1].split('.')[0].lower()])\n",
        "view = spectral.imshow(X,(30,20,100), classes=y,figsize=(9,9))\n",
        "view.set_display_mode('overlay')\n",
        "view.class_alpha = 0.5\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DkOPo_0eqpgC",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzYZw5UGN1uN",
        "colab_type": "code",
        "outputId": "140ec76b-ee86-464f-86df-33c0e5615cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "\n",
        "for param in danet_model.parameters():\n",
        "    print(param.shape)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1])\n",
            "torch.Size([25, 200, 1, 1])\n",
            "torch.Size([25])\n",
            "torch.Size([25, 200, 1, 1])\n",
            "torch.Size([25])\n",
            "torch.Size([200, 200, 1, 1])\n",
            "torch.Size([200])\n",
            "torch.Size([1])\n",
            "torch.Size([24, 1, 24, 3, 3])\n",
            "torch.Size([24])\n",
            "torch.Size([24])\n",
            "torch.Size([24])\n",
            "torch.Size([1])\n",
            "torch.Size([48, 24, 24, 3, 3])\n",
            "torch.Size([48])\n",
            "torch.Size([48])\n",
            "torch.Size([48])\n",
            "torch.Size([1])\n",
            "torch.Size([48, 24, 9, 3, 3])\n",
            "torch.Size([24])\n",
            "torch.Size([24])\n",
            "torch.Size([24])\n",
            "torch.Size([1])\n",
            "torch.Size([24, 1, 38, 3, 3])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79u5nqahClcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"class PAM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PAM,self).__init__()\n",
        "    self.B = nn.Sequential(nn.Conv2d(200, 64, (3, 3), 1, 0),\n",
        "                                   nn.ReLU(True))\n",
        "    self.C = nn.Sequential(nn.Conv2d(200, 64, (3, 3), 1, 0),\n",
        "                                   nn.ReLU(True))\n",
        "    self.D = nn.Sequential(nn.Conv2d(200, 64, (3, 3), 1, 0),\n",
        "                                   nn.ReLU(True))\n",
        "    self.soft = nn.Softmax2d()\n",
        "    self.alpha = nn.Variable(torch.ones(1, 1), requires_grad=True)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    A = x # C H W \n",
        "    b = self.B(x) # C H W \n",
        "    c = self.C(x) # C H W \n",
        "    d = self.D(x) # C H W \n",
        "    \n",
        "    b = b.view(-1,b.size()[2]*b.size()[3]).T # N C \n",
        "    c = c.view(-1,c.size()[2]*c.size()[3]) #C N\n",
        "    d = d.view(-1,d.size()[2]*d.size()[3]) #C N\n",
        "    S = self.soft(torch.mm(b,c)) # N N\n",
        "    sd = torch.mm(d,S) # C N \n",
        "    sd = sd.view(-1,x.size()[2],x.size()[3])\n",
        "    E = torch.add(self.alpha*sd , A)\n",
        "    \n",
        "    return E\n",
        "    \n",
        "    \n",
        "        \n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkMhnFofHk3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"class CAM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CAM,self).__init__()\n",
        "    self.soft = nn.Softmax2d()\n",
        "    \n",
        "  def forward(self,x):\n",
        "    # x is C * H * W\n",
        "    y = x.view(-1,x.size()[2]*x.size()[3]) #C * N\n",
        "    X = self.soft(torch.mm(y,y.T))   # C * C\n",
        "    Xy = torch.mm(X,y)     # C * N\n",
        "    Xy = Xy.view(-1,x.size()[2],x.size()[3]) #C H W\n",
        "    E = torch.add(Xy,x) # C H W\n",
        "    \n",
        "    \n",
        "    \n",
        "    return E\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0uz-TUZ1guQm",
        "colab": {}
      },
      "source": [
        "\"\"\"import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "X, y = createImageCubes(X, y, windowSize=15)\n",
        "def plot(r):\n",
        "  assert r<=10000\n",
        "  fig, axes = plt.subplots(32, 32, figsize=(20, 20))\n",
        "  itera = [*range(r)]\n",
        "  for t,ax in zip(itera,axes.flatten()):\n",
        "    ax.imshow(X[t,:,:,0])\n",
        "    plt.subplots_adjust(wspace=.5, hspace=.5)\n",
        "plot(1000)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FJMnXE_ViLAD",
        "colab": {}
      },
      "source": [
        "def visualize_tile():\n",
        "    with torch.no_grad():\n",
        "        # Get a batch of training data\n",
        "        data = next(iter(train_loader))[0].to(device)\n",
        "      \n",
        "        input_tensor = data.cpu().numpy()\n",
        "        \n",
        "        transformed_input_tensor = model.RecNet(data).cpu().numpy()\n",
        "        \n",
        "\n",
        "\n",
        "        # Plot the results side-by-side\n",
        "        f, axarr = plt.subplots(1, 2,figsize=(10,10))\n",
        "        axarr[0].imshow(input_tensor[0,0,:,:],cmap='gnuplot')\n",
        "        axarr[0].set_title('Dataset Images')\n",
        "\n",
        "        axarr[1].imshow(transformed_tensor[0,0,:,:],cmap='gnuplot')\n",
        "        axarr[1].set_title('Transformed Images')\n",
        "\n",
        "#visualize_tile()\n",
        "\n",
        "#plt.ioff()\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6G2bi-vQpY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}